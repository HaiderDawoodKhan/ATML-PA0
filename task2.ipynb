{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b31b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoConfig, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e496e9e",
   "metadata": {},
   "source": [
    "### Using a Pre-trained ViT for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e015a5c",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/deit-small-patch16-224\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21950851",
   "metadata": {},
   "source": [
    "Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_URLS = [\n",
    "    \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\",  # cat\n",
    "    \"https://images.unsplash.com/photo-1518020382113-a7e8fc38eac9\",  # dog\n",
    "    \"https://images.unsplash.com/photo-1519681393784-d120267933ba\",  # mountains\n",
    "]\n",
    "\n",
    "def is_url(s: str) -> bool:\n",
    "    return s.startswith(\"http://\") or s.startswith(\"https://\")\n",
    "\n",
    "def load_image_from_url(url: str, timeout: int = 10) -> Image.Image:\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "\n",
    "def load_image_from_path(p: str) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "def load_images(sources: List[str]) -> List[Tuple[str, Image.Image]]:\n",
    "    images = []\n",
    "    for src in sources:\n",
    "        try:\n",
    "            if is_url(src):\n",
    "                img = load_image_from_url(src)\n",
    "                name = src.split(\"/\")[-1] or \"image\"\n",
    "            else:\n",
    "                img = load_image_from_path(src)\n",
    "                name = pathlib.Path(src).stem\n",
    "            images.append((name, img))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load '{src}': {e}\", file=sys.stderr)\n",
    "    return images\n",
    "\n",
    "\n",
    "sources = DEFAULT_IMAGE_URLS\n",
    "items = load_images(sources)\n",
    "\n",
    "pil_images = [img for (_, img) in items]\n",
    "\n",
    "inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aea212",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ad21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "    logits = outputs.logits                \n",
    "    attentions = outputs.attentions  \n",
    "\n",
    "probs = logits.softmax(dim=-1)\n",
    "top1_ids = probs.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bef959",
   "metadata": {},
   "source": [
    "### Visualizing Patch Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2046be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_01(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    mn, mx = x.min(), x.max()\n",
    "    return (x - mn) / (mx - mn + eps)\n",
    "\n",
    "def get_cls_to_patches_attention(\n",
    "    attentions: List[torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract CLS -> patch attentions from the last layer and average across heads.\n",
    "\n",
    "    attentions: list[length=num_layers] of tensors with shape (B, H, L, L)\n",
    "    Returns: tensor with shape (B, num_patches), values in [0,1] (normalized per image).\n",
    "    \"\"\"\n",
    "    last = attentions[-1]              # (B, H, L, L)\n",
    "    B, H, L, _ = last.shape\n",
    "    # token 0 is CLS. We take CLS -> others (dim=-1 indexes the \"to\" tokens).\n",
    "    # Exclude CLS->CLS by slicing 1: for patches only.\n",
    "    cls_to_all = last[:, :, 0, 1:]     # (B, H, L-1)\n",
    "    # Average across heads:\n",
    "    cls_avg = cls_to_all.mean(dim=1)   # (B, L-1) = (B, num_patches)\n",
    "\n",
    "    # Normalize per sample for nicer visualization:\n",
    "    out = torch.stack([normalize_to_01(cls_avg[b]) for b in range(B)], dim=0)\n",
    "    return out  # (B, num_patches) in [0,1]\n",
    "\n",
    "\n",
    "cls_patch_attn = get_cls_to_patches_attention(attentions)  # (B, num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ec13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p: str):\n",
    "    pathlib.Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "def attention_map_to_heatmap(\n",
    "    attn_vec: torch.Tensor,\n",
    "    ref_img_size: Tuple[int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a flat attention vector (num_patches) to an upsampled heatmap (H,W).\n",
    "    Assumes a square patch grid (e.g., 14x14 for 224 with 16x16 patches).\n",
    "    \"\"\"\n",
    "    num_patches = attn_vec.numel()\n",
    "    grid = int(math.sqrt(num_patches))\n",
    "    assert grid * grid == num_patches, f\"num_patches={num_patches} is not a perfect square.\"\n",
    "\n",
    "    # Reshape to grid\n",
    "    attn_grid = attn_vec.reshape(grid, grid).detach().cpu().numpy()\n",
    "\n",
    "    # Upsample to image size using PIL\n",
    "    attn_img = Image.fromarray((attn_grid * 255).astype(np.uint8), mode=\"L\")\n",
    "    attn_img = attn_img.resize(ref_img_size, resample=Image.BICUBIC)\n",
    "    return np.array(attn_img).astype(np.float32) / 255.0  # back to [0,1]\n",
    "\n",
    "\n",
    "def overlay_heatmap_on_image(\n",
    "    image: Image.Image,\n",
    "    heatmap: np.ndarray,\n",
    "    alpha: float = 0.45,\n",
    "    cmap: str = \"jet\",\n",
    "    save_path: str = None,\n",
    "    title: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Show and optionally save an overlay of the heatmap on the image.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(heatmap, cmap=cmap, alpha=alpha, interpolation=\"bilinear\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if save_path:\n",
    "        ensure_dir(save_path)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n",
    "        print(f\"[INFO] Saved attention overlay → {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "target_size = pil_images[0].size  # (W, H) for overlay sizing\n",
    "\n",
    "for i, ((name, pil_img), idx) in enumerate(zip(items, top1_ids)):\n",
    "    label = id2label.get(idx, f\"class_{idx}\")\n",
    "    conf = probs[i, idx].item()\n",
    "    print(f\"[RESULT] {name}: Top-1 = {label} (p={conf:.3f})\")\n",
    "\n",
    "    # Build upsampled heatmap for this image\n",
    "    heatmap = attention_map_to_heatmap(cls_patch_attn[i], (pil_img.size[0], pil_img.size[1]))\n",
    "\n",
    "    # Overlay & save\n",
    "    out_name = f\"{name}__attn.png\"\n",
    "    overlay_title = f\"{label} (p={conf:.2f}) — CLS attention\"\n",
    "    overlay_heatmap_on_image(pil_img, heatmap, alpha=0.45, cmap=\"jet\", save_path=out_name, title=overlay_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ab27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# vit_attn_from_medium.py\n",
    "# ------------------------------------------------------------\n",
    "# Implements:\n",
    "# (1) Top-1 classification with a small ImageNet-pretrained ViT-family model\n",
    "# (2) Patch attention visualization using ATTENTION ROLLOUT exactly as in the linked post:\n",
    "#     - fuse heads by mean\n",
    "#     - add identity (A + I)\n",
    "#     - row-normalize\n",
    "#     - multiply across layers\n",
    "#     - extract a CLS↔patch vector, reshape to grid, normalize, blur, and overlay\n",
    "#\n",
    "# Usage:\n",
    "#   python vit_attn_from_medium.py --images cat.jpg https://.../dog.png\n",
    "#   python vit_attn_from_medium.py                  # uses 3 default URLs\n",
    "#\n",
    "# Output:\n",
    "#   - Prints Top-1 predictions (class + probability)\n",
    "#   - Saves `<name>__attn.png` heatmap overlays\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    ViTForImageClassification,\n",
    ")\n",
    "\n",
    "# You can switch to \"google/vit-base-patch16-224\" if you prefer canonical ViT.\n",
    "MODEL_NAME = \"facebook/deit-small-patch16-224\"\n",
    "\n",
    "DEFAULT_IMAGE_URLS = [\n",
    "    \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\",  # cat\n",
    "    \"https://images.unsplash.com/photo-1518020382113-a7e8fc38eac9\",  # dog\n",
    "    \"https://images.unsplash.com/photo-1519681393784-d120267933ba\",  # coffee\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------- IO helpers -----------------------\n",
    "\n",
    "def is_url(s: str) -> bool:\n",
    "    return s.startswith(\"http://\") or s.startswith(\"https://\")\n",
    "\n",
    "def load_image_from_url(url: str, timeout: int = 12) -> Image.Image:\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "\n",
    "def load_image_from_path(p: str) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "def load_images(sources: List[str]) -> List[Tuple[str, Image.Image]]:\n",
    "    images = []\n",
    "    for src in sources:\n",
    "        try:\n",
    "            img = load_image_from_url(src) if is_url(src) else load_image_from_path(src)\n",
    "            name = (src.split(\"/\")[-1] or \"image\") if is_url(src) else pathlib.Path(src).stem\n",
    "            images.append((name, img))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load '{src}': {e}\", file=sys.stderr)\n",
    "    return images\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    pathlib.Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------- Attention rollout (from the article) -----------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def attention_rollout(attentions: List[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the \"rolling attention\" described in the article:\n",
    "      - Fuse heads with mean\n",
    "      - Add identity (A + I)\n",
    "      - Row-normalize\n",
    "      - Multiply across layers\n",
    "\n",
    "    attentions: list of length L_layers, each tensor (B, H, T, T)\n",
    "    returns: rollout (B, T, T)\n",
    "    \"\"\"\n",
    "    # Start with identity\n",
    "    T = attentions[0].size(-1)\n",
    "    device = attentions[0].device\n",
    "    rollout = torch.eye(T, device=device).unsqueeze(0)  # (1, T, T)\n",
    "\n",
    "    for A in attentions:\n",
    "        # mean over heads -> (B, T, T)\n",
    "        A_fused = A.mean(dim=1)\n",
    "        # add identity\n",
    "        I = torch.eye(T, device=A_fused.device).unsqueeze(0)  # (1, T, T)\n",
    "        A_aug = A_fused + I\n",
    "        # row-normalize\n",
    "        A_norm = A_aug / A_aug.sum(dim=-1, keepdim=True)\n",
    "        # multiply (broadcast batch): (B,T,T) = (B,T,T) @ (B,T,T) via per-sample matmul\n",
    "        rollout = rollout @ A_norm\n",
    "\n",
    "    return rollout  # (B, T, T)\n",
    "\n",
    "\n",
    "def cls_to_patches_from_rollout(rollout: torch.Tensor, follow_medium_indexing: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract a CLS↔patch vector from rollout.\n",
    "\n",
    "    The Medium post indexes like: rollout[0, 1:, 0]\n",
    "    We'll support that *and* the more common 'row=CLS -> cols=patches' (rollout[0, 0, 1:]).\n",
    "    By default, we follow the Medium post to honor the user's request.\n",
    "    \"\"\"\n",
    "    # rollout: (B, T, T), where T = 1 + num_patches\n",
    "    if follow_medium_indexing:\n",
    "        vec = rollout[:, 1:, 0]  # (B, num_patches)\n",
    "        # The article inverted then normalized; we keep that behavior:\n",
    "        vec = 1.0 - vec\n",
    "    else:\n",
    "        vec = rollout[:, 0, 1:]  # (B, num_patches)\n",
    "\n",
    "    # Normalize each sample to [0,1] for visualization\n",
    "    vec = (vec - vec.amin(dim=1, keepdim=True)) / (vec.amax(dim=1, keepdim=True) - vec.amin(dim=1, keepdim=True) + 1e-6)\n",
    "    return vec  # (B, num_patches) in [0,1]\n",
    "\n",
    "\n",
    "# ----------------------- Viz helpers -----------------------\n",
    "\n",
    "def upsample_attention_to_image(attn_vec: torch.Tensor, target_size: Tuple[int, int]) -> Image.Image:\n",
    "    \"\"\"\n",
    "    attn_vec: (num_patches,) torch tensor in [0,1]\n",
    "    target_size: (W, H)\n",
    "    returns a PIL Image (mode 'L') resized to target_size\n",
    "    \"\"\"\n",
    "    num_patches = attn_vec.numel()\n",
    "    grid = int(math.sqrt(num_patches))\n",
    "    assert grid * grid == num_patches, f\"num_patches={num_patches} is not a square\"\n",
    "    arr = (attn_vec.detach().cpu().numpy().reshape(grid, grid) * 255).astype(np.uint8)\n",
    "    heat = Image.fromarray(arr, mode=\"L\")\n",
    "    heat = heat.resize(target_size, resample=Image.BICUBIC)\n",
    "    # Optional smoothing as in the article\n",
    "    heat = heat.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    return heat  # grayscale (L)\n",
    "\n",
    "def overlay_heatmap_rgba(image: Image.Image, heat_gray: Image.Image, alpha: int = 100, save_path: str = None, title: str = None):\n",
    "    \"\"\"\n",
    "    Builds an RGBA overlay from grayscale (alpha channel = intensity) and blends it on 'image'.\n",
    "    \"\"\"\n",
    "    # Convert to RGBA with alpha proportional to heat (as in the article)\n",
    "    heat_arr = np.array(heat_gray.convert(\"L\"))\n",
    "    rgba = np.stack([heat_arr, heat_arr, heat_arr, heat_arr], axis=-1)  # grayscale in RGB, same as alpha seed\n",
    "    heat_rgba = Image.fromarray(rgba, mode=\"RGBA\")\n",
    "    heat_rgba.putalpha(alpha)  # control opacity\n",
    "\n",
    "    # Compose\n",
    "    base = image.convert(\"RGBA\")\n",
    "    composed = Image.alpha_composite(base, heat_rgba)\n",
    "\n",
    "    # Matplotlib save with optional title (keeps things simple)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(composed)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    if save_path:\n",
    "        ensure_dir(save_path)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n",
    "        print(f\"[INFO] Saved → {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def main(args):\n",
    "    # Load model + processor\n",
    "    print(f\"[INFO] Loading model: {MODEL_NAME}\")\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "    try:\n",
    "        model = ViTForImageClassification.from_pretrained(MODEL_NAME, attn_implementation=\"eager\")\n",
    "    except TypeError:\n",
    "        # Older transformers versions may not accept this kwarg\n",
    "        model = ViTForImageClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Prepare images\n",
    "    sources = args.images if args.images else DEFAULT_IMAGE_URLS\n",
    "    items = load_images(sources)\n",
    "    if not items:\n",
    "        print(\"[ERROR] No images loaded.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    pil_images = [img for (_, img) in items]\n",
    "    inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward with attentions\n",
    "    outputs = model(**inputs, output_attentions=True, return_dict=True)\n",
    "    logits = outputs.logits                         # (B, 1000)\n",
    "    attentions = outputs.attentions                 # list[L_layers] of (B, H, T, T)\n",
    "\n",
    "    # Top-1 predictions\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    top1_ids = probs.argmax(dim=-1).tolist()\n",
    "    id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "\n",
    "    for i, ((name, img), cls_id) in enumerate(zip(items, top1_ids)):\n",
    "        label = id2label.get(cls_id, f\"class_{cls_id}\")\n",
    "        conf = probs[i, cls_id].item()\n",
    "        print(f\"[RESULT] {name}: Top-1 = {label} (p={conf:.3f})\")\n",
    "\n",
    "    # Attention rollout (per article)\n",
    "    rollout = attention_rollout(attentions)         # (B, T, T)\n",
    "\n",
    "    # CLS↔patch vector (follow the article’s indexing by default)\n",
    "    cls_vecs = cls_to_patches_from_rollout(rollout, follow_medium_indexing=True)  # (B, num_patches)\n",
    "\n",
    "    # Build overlays\n",
    "    for i, (name, img) in enumerate(items):\n",
    "        heat_L = upsample_attention_to_image(cls_vecs[i], (img.size[0], img.size[1]))  # grayscale 'L'\n",
    "        title = f\"{id2label.get(top1_ids[i], str(top1_ids[i]))} (p={probs[i, top1_ids[i]].item():.2f}) — attention rollout\"\n",
    "        out_path = f\"{name}__attn.png\"\n",
    "        overlay_heatmap_rgba(img, heat_L, alpha=100, save_path=out_path, title=title)\n",
    "\n",
    "    print(\"\\n[NOTE] Check the saved *_attn.png files; do the highlighted regions match the object?\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"ViT classification + attention visualization (Medium-style rollout).\")\n",
    "    parser.add_argument(\"--images\", nargs=\"*\", help=\"Paths or URLs to 1–3 images.\")\n",
    "    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"Force CPU.\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
