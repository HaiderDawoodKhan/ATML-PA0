{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27005d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4b85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "stl10_train = torchvision.datasets.STL10(root='./stl_data', split='train', download=True, transform=transform)\n",
    "stl10_test = torchvision.datasets.STL10(root='./stl_data', split='test', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class STL10CLIPEvaluator:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load CLIP model\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        print(\"CLIP model loaded successfully\")\n",
    "        \n",
    "        # STL-10 class names\n",
    "        self.class_names = [\n",
    "            \"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \n",
    "            \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"\n",
    "        ]\n",
    "        \n",
    "    def load_stl10_data(self, batch_size=64):\n",
    "        \"\"\"Load STL-10 test dataset\"\"\"\n",
    "        test_loader = DataLoader(\n",
    "            stl10_test, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "        print(f\"STL-10 test set loaded: {len(test_dataset)} samples\")\n",
    "        return test_loader\n",
    "    \n",
    "    def create_text_prompts(self, strategy=\"plain\"):\n",
    "        \"\"\"Create text prompts based on different strategies\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        if strategy == \"plain\":\n",
    "            # Strategy 1: Plain labels\n",
    "            prompts = self.class_names\n",
    "            \n",
    "        elif strategy == \"simple_template\":\n",
    "            # Strategy 2: Simple template\n",
    "            template = \"a photo of a {}\"\n",
    "            prompts = [template.format(class_name) for class_name in self.class_names]\n",
    "            \n",
    "        elif strategy == \"descriptive\":\n",
    "            # Strategy 3: More descriptive prompts\n",
    "            descriptive_prompts = {\n",
    "                \"airplane\": \"a photo of an airplane flying in the sky\",\n",
    "                \"bird\": \"a photo of a colorful bird with wings\",\n",
    "                \"car\": \"a photo of a modern car on the road\",\n",
    "                \"cat\": \"a photo of a cute domestic cat with fur\",\n",
    "                \"deer\": \"a photo of a graceful deer in nature\",\n",
    "                \"dog\": \"a photo of a friendly dog with a wagging tail\",\n",
    "                \"horse\": \"a photo of a majestic horse running\",\n",
    "                \"monkey\": \"a photo of a playful monkey in trees\",\n",
    "                \"ship\": \"a photo of a large ship sailing on water\",\n",
    "                \"truck\": \"a photo of a heavy truck on highway\"\n",
    "            }\n",
    "            prompts = [descriptive_prompts[class_name] for class_name in self.class_names]\n",
    "            \n",
    "        elif strategy == \"context_rich\":\n",
    "            # Strategy 4: Context-rich prompts\n",
    "            context_prompts = {\n",
    "                \"airplane\": \"a high-quality photograph of a commercial airplane during flight\",\n",
    "                \"bird\": \"a detailed image of a wild bird with beautiful plumage\",\n",
    "                \"car\": \"a clear photograph of an automobile vehicle\",\n",
    "                \"cat\": \"a portrait of a house cat with whiskers and pointed ears\",\n",
    "                \"deer\": \"a wildlife photograph of a deer with antlers in forest\",\n",
    "                \"dog\": \"a photograph of a domestic dog breed with four legs\",\n",
    "                \"horse\": \"an image of an equine animal with a mane\",\n",
    "                \"monkey\": \"a photograph of a primate swinging through trees\",\n",
    "                \"ship\": \"a maritime photograph of a vessel on ocean waters\",\n",
    "                \"truck\": \"a photograph of a large motor vehicle for cargo transport\"\n",
    "            }\n",
    "            prompts = [context_prompts[class_name] for class_name in self.class_names]\n",
    "            \n",
    "        return prompts\n",
    "    \n",
    "    def evaluate_zero_shot(self, test_loader, prompting_strategy=\"plain\"):\n",
    "        \"\"\"Evaluate CLIP with zero-shot classification\"\"\"\n",
    "        print(f\"\\nEvaluating with '{prompting_strategy}' prompting strategy...\")\n",
    "        \n",
    "        # Create text prompts\n",
    "        text_prompts = self.create_text_prompts(prompting_strategy)\n",
    "        print(\"Text prompts:\")\n",
    "        for i, prompt in enumerate(text_prompts):\n",
    "            print(f\"  {self.class_names[i]}: '{prompt}'\")\n",
    "        \n",
    "        # Encode text prompts\n",
    "        text_tokens = clip.tokenize(text_prompts).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.encode_text(text_tokens)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                # Encode images\n",
    "                image_features = self.model.encode_image(images)\n",
    "                image_features = F.normalize(image_features, dim=-1)\n",
    "                \n",
    "                # Calculate similarity scores\n",
    "                logits = image_features @ text_features.T\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print(f\"Accuracy with '{prompting_strategy}': {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return accuracy, all_predictions, all_labels\n",
    "    \n",
    "    def run_comprehensive_evaluation(self):\n",
    "        \"\"\"Run evaluation with multiple prompting strategies\"\"\"\n",
    "        # Load data\n",
    "        test_loader = self.load_stl10_data()\n",
    "        \n",
    "        strategies = [\"plain\", \"simple_template\", \"descriptive\", \"context_rich\"]\n",
    "        results = {}\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"CLIP Zero-Shot Evaluation on STL-10\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            accuracy, predictions, labels = self.evaluate_zero_shot(test_loader, strategy)\n",
    "            results[strategy] = {\n",
    "                'accuracy': accuracy,\n",
    "                'predictions': predictions,\n",
    "                'labels': labels\n",
    "            }\n",
    "        \n",
    "        # Display comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for strategy, result in results.items():\n",
    "            print(f\"{strategy:20s}: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
    "        \n",
    "        # Find best strategy\n",
    "        best_strategy = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "        best_accuracy = results[best_strategy]['accuracy']\n",
    "        \n",
    "        print(f\"\\nBest strategy: {best_strategy} with {best_accuracy:.4f} accuracy\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_per_class_performance(self, results):\n",
    "        \"\"\"Analyze per-class performance for different strategies\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PER-CLASS ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for strategy, result in results.items():\n",
    "            print(f\"\\n{strategy.upper()} Strategy:\")\n",
    "            predictions = np.array(result['predictions'])\n",
    "            labels = np.array(result['labels'])\n",
    "            \n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                class_mask = labels == i\n",
    "                class_predictions = predictions[class_mask]\n",
    "                class_accuracy = (class_predictions == i).mean() if class_mask.sum() > 0 else 0\n",
    "                print(f\"  {class_name:10s}: {class_accuracy:.3f}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize evaluator\n",
    "    evaluator = STL10CLIPEvaluator()\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    results = evaluator.run_comprehensive_evaluation()\n",
    "    \n",
    "    # Analyze per-class performance\n",
    "    evaluator.analyze_per_class_performance(results)\n",
    "    \n",
    "    # Create visualization\n",
    "    create_results_visualization(results)\n",
    "\n",
    "def create_results_visualization(results):\n",
    "    \"\"\"Create a bar chart comparing different prompting strategies\"\"\"\n",
    "    strategies = list(results.keys())\n",
    "    accuracies = [results[strategy]['accuracy'] for strategy in strategies]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(strategies, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, accuracy in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{accuracy:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('CLIP Zero-Shot Accuracy on STL-10 with Different Prompting Strategies')\n",
    "    plt.xlabel('Prompting Strategy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, max(accuracies) + 0.05)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('clip_stl10_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/CLIP.git torchvision torch umap-learn scikit-learn matplotlib\n",
    "import random, torch, clip, numpy as np\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAVE_UMAP = True\n",
    "except Exception:\n",
    "    HAVE_UMAP = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# --- Load CLIP ---\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "# --- STL-10 classes (match dataset labels) ---\n",
    "classes = [\"airplane\",\"bird\",\"car\",\"cat\",\"deer\",\"dog\",\"horse\",\"monkey\",\"ship\",\"truck\"]\n",
    "\n",
    "# --- Build your prompt set (you can tweak/expand) ---\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a bright photo of a {}\",\n",
    "    \"a picture of a {}\"\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_prototypes(classnames, templates):\n",
    "    \"\"\"Return [C, D] normalized text prototypes for each class (prompt-ensemble averaged).\"\"\"\n",
    "    zs = []\n",
    "    for cname in classnames:\n",
    "        texts = [t.format(cname) for t in templates]\n",
    "        tok = clip.tokenize(texts).to(DEVICE)\n",
    "        te = model.encode_text(tok)                # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)    # L2 per prompt\n",
    "        te = te.mean(dim=0)                        # average prompts\n",
    "        te = te / te.norm()                        # final normalized prototype\n",
    "        zs.append(te)\n",
    "    return torch.stack(zs, dim=0)                  # [C, D]\n",
    "\n",
    "# --- Data: take 50-100 samples from STL-10 test ---\n",
    "FULL_TEST = datasets.STL10(root=\"./data\", split=\"test\", download=True, transform=preprocess)\n",
    "N = 100  # set 50..100\n",
    "idxs = np.random.choice(len(FULL_TEST), size=N, replace=False)\n",
    "TEST = Subset(FULL_TEST, idxs)\n",
    "loader = DataLoader(TEST, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- Extract embeddings ---\n",
    "@torch.no_grad()\n",
    "def collect_embeddings():\n",
    "    text_protos = build_text_prototypes(classes, TEMPLATES)   # [C, D], L2-normalized\n",
    "    X_img, X_txt, y = [], [], []\n",
    "\n",
    "    for ims, labels in loader:\n",
    "        ims = ims.to(DEVICE); labels = labels.to(DEVICE)\n",
    "        z_img = model.encode_image(ims)                       # [B, D]\n",
    "        # raw (un-normalized) image feats kept; we’ll also store normalized versions\n",
    "        X_img.append(z_img.detach().cpu().numpy())\n",
    "        # For each label, pick the corresponding text prototype (one per class)\n",
    "        X_txt.append(text_protos[labels].detach().cpu().numpy())\n",
    "        y.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    X_img = np.concatenate(X_img, axis=0)  # [N, D]\n",
    "    X_txt = np.concatenate(X_txt, axis=0)  # [N, D] (repeated class prototypes per sample)\n",
    "    y = np.concatenate(y, axis=0)          # [N]\n",
    "    return X_img, X_txt, y, text_protos.cpu().numpy()\n",
    "\n",
    "X_img_raw, X_txt_norm, y, text_protos_norm = collect_embeddings()\n",
    "# Normalize image embeddings as well (direction-only view)\n",
    "X_img_norm = X_img_raw / np.linalg.norm(X_img_raw, axis=1, keepdims=True)\n",
    "\n",
    "# --- Simple quantitative “gap” metrics ---\n",
    "def cosine(a, b):\n",
    "    a = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b = b / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return np.sum(a*b, axis=1)\n",
    "\n",
    "# Pair each image with its matching class prototype:\n",
    "paired_txt = text_protos_norm[y]                 # [N, D]\n",
    "pos_cos_raw = cosine(X_img_raw, paired_txt)      # BEFORE L2 on images\n",
    "pos_cos_norm = cosine(X_img_norm, paired_txt)    # AFTER L2 on images\n",
    "\n",
    "# Imposter negatives: match each image to a prototype from a *different* class\n",
    "rng = np.random.default_rng(SEED)\n",
    "neg_labels = np.array([rng.choice([c for c in range(10) if c != yy]) for yy in y])\n",
    "neg_txt = text_protos_norm[neg_labels]\n",
    "neg_cos_norm = cosine(X_img_norm, neg_txt)\n",
    "\n",
    "print(\"\\n=== Modality gap quick metrics ===\")\n",
    "print(f\"Mean cosine (image raw  vs matched text): {pos_cos_raw.mean():.3f}\")\n",
    "print(f\"Mean cosine (image L2   vs matched text): {pos_cos_norm.mean():.3f}\")\n",
    "print(f\"Mean cosine (image L2   vs mismatched  ): {neg_cos_norm.mean():.3f}\")\n",
    "print(f\"Avg margin (pos - neg, normalized): {(pos_cos_norm - neg_cos_norm).mean():.3f}\")\n",
    "\n",
    "# Angle between global means (another coarse proxy)\n",
    "mean_img_raw  = X_img_raw.mean(axis=0);  mean_img_raw  /= np.linalg.norm(mean_img_raw)\n",
    "mean_img_norm = X_img_norm.mean(axis=0); mean_img_norm /= np.linalg.norm(mean_img_norm)\n",
    "mean_txt = text_protos_norm.mean(axis=0); mean_txt /= np.linalg.norm(mean_txt)\n",
    "print(f\"Cos(mean_img_raw,  mean_text): {float(mean_img_raw @ mean_txt):.3f}\")\n",
    "print(f\"Cos(mean_img_norm, mean_text): {float(mean_img_norm @ mean_txt):.3f}\")\n",
    "\n",
    "# --- 2D projection helper (UMAP or TSNE). Optionally PCA->TSNE for stability ---\n",
    "def project_2d(X, method=\"umap\"):\n",
    "    if method == \"umap\" and HAVE_UMAP:\n",
    "        reducer = umap.UMAP(n_neighbors=20, min_dist=0.1, metric=\"cosine\", random_state=SEED)\n",
    "        return reducer.fit_transform(X)\n",
    "    # t-SNE fallback (run on PCA-50 first for speed/stability)\n",
    "    Xp = PCA(n_components=min(50, X.shape[1]), random_state=SEED).fit_transform(X)\n",
    "    tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=SEED)\n",
    "    return tsne.fit_transform(Xp)\n",
    "\n",
    "# --- Visualize: per-sample images vs their (repeated) text prototypes ---\n",
    "def plot_overlay(Z_img, Z_txt, labels, title, savepath=None):\n",
    "    plt.figure(figsize=(7.0, 6.3))\n",
    "    # color by class; circles=image, x=text\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "    for c in range(10):\n",
    "        m_img = labels==c\n",
    "        m_txt = labels==c\n",
    "        plt.scatter(Z_img[m_img,0], Z_img[m_img,1], s=14, alpha=0.7, label=f\"{classes[c]} (img)\", c=[cmap(c)])\n",
    "        plt.scatter(Z_txt[m_txt,0], Z_txt[m_txt,1], s=28, alpha=0.9, marker=\"x\", label=f\"{classes[c]} (text)\", c=[cmap(c)])\n",
    "    plt.title(title)\n",
    "    plt.legend(ncol=2, fontsize=8, frameon=False)\n",
    "    plt.tight_layout()\n",
    "    if savepath: plt.savefig(savepath, dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "# --- Project and plot (RAW vs L2-normalized image feats) ---\n",
    "Z_img_raw  = project_2d(X_img_raw,  method=\"umap\")\n",
    "Z_img_norm = project_2d(X_img_norm, method=\"umap\")\n",
    "Z_txt_rep  = project_2d(X_txt_norm, method=\"umap\")  # repeated class prototypes (one per sample)\n",
    "\n",
    "plot_overlay(Z_img_raw,  Z_txt_rep, y, title=\"CLIP Embeddings (RAW image feats vs text)\")\n",
    "plot_overlay(Z_img_norm, Z_txt_rep, y, title=\"CLIP Embeddings (L2-normalized image feats vs text)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f75d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/CLIP.git torch torchvision scipy umap-learn scikit-learn matplotlib\n",
    "import numpy as np, torch, clip, random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAVE_UMAP = True\n",
    "except Exception:\n",
    "    HAVE_UMAP = False\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"ViT-B/32\"\n",
    "BATCH = 256\n",
    "FIT_SIZE = 100     # use 50-100 samples to fit R (per the assignment)\n",
    "USE_UMAP = True    # set False to force t-SNE\n",
    "# --------------------------------------------\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# STL-10 classes (order matches dataset targets)\n",
    "CLASSES = [\"airplane\",\"bird\",\"car\",\"cat\",\"deer\",\"dog\",\"horse\",\"monkey\",\"ship\",\"truck\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a bright photo of a {}\",\n",
    "    \"a picture of a {}\"\n",
    "]\n",
    "\n",
    "# ---------- Load CLIP -----------\n",
    "model, preprocess = clip.load(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# ---------- Data -----------\n",
    "test_set = datasets.STL10(root=\"./data\", split=\"test\", download=True, transform=preprocess)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ---------- Text prototypes (prompt ensemble) -----------\n",
    "@torch.no_grad()\n",
    "def build_text_prototypes(classnames, templates):\n",
    "    Z = []\n",
    "    for cname in classnames:\n",
    "        texts = [t.format(cname) for t in templates]\n",
    "        tok = clip.tokenize(texts).to(DEVICE)\n",
    "        te = model.encode_text(tok)                # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        proto = te.mean(dim=0)\n",
    "        proto = proto / proto.norm()\n",
    "        Z.append(proto)\n",
    "    return torch.stack(Z, dim=0)                   # [C, D], L2-normalized\n",
    "\n",
    "# ---------- Collect all image feats + labels, and per-sample matched text protos -----------\n",
    "@torch.no_grad()\n",
    "def collect_all():\n",
    "    text_protos = build_text_prototypes(CLASSES, TEMPLATES)    # [C, D], L2\n",
    "    X_img, y = [], []\n",
    "    for ims, labels in test_loader:\n",
    "        ims = ims.to(DEVICE)\n",
    "        z = model.encode_image(ims)                             # [B, D] (raw)\n",
    "        X_img.append(z.cpu().numpy())\n",
    "        y.append(labels.numpy())\n",
    "    X_img = np.concatenate(X_img, axis=0)                      # [N, D]\n",
    "    y = np.concatenate(y, axis=0)                              # [N]\n",
    "    X_txt = text_protos.cpu().numpy()[y]                       # [N, D], repeated class proto\n",
    "    # Also keep the per-class matrix once (not repeated)\n",
    "    class_text = text_protos.cpu().numpy()                     # [C, D]\n",
    "    return X_img, X_txt, y, class_text\n",
    "\n",
    "X_img_raw, X_txt_norm_rep, y, class_text_norm = collect_all()\n",
    "N, D = X_img_raw.shape\n",
    "\n",
    "# L2-normalized image features (cosine space)\n",
    "X_img_norm = X_img_raw / np.linalg.norm(X_img_raw, axis=1, keepdims=True)\n",
    "\n",
    "# ---------- (a)-(c) Learn orthogonal R via Procrustes on a small FIT subset ----------\n",
    "# Pair images with their *matched* text (same label). We’ll fit on normalized rows.\n",
    "fit_idx = np.random.choice(N, size=min(FIT_SIZE, N), replace=False)\n",
    "X_fit = X_img_norm[fit_idx]                      # [K, D]\n",
    "Y_fit = X_txt_norm_rep[fit_idx]                  # [K, D] (matched text prototypes)\n",
    "\n",
    "# Optional mean-centering (classical Procrustes with translation); try both:\n",
    "def orthogonal_R(X, Y, center=True):\n",
    "    Xc, Yc = X.copy(), Y.copy()\n",
    "    if center:\n",
    "        Xc -= Xc.mean(axis=0, keepdims=True)\n",
    "        Yc -= Yc.mean(axis=0, keepdims=True)\n",
    "    R, _ = orthogonal_procrustes(Xc, Yc)         # solve min ||X R - Y||_F s.t. R^T R = I\n",
    "    return R\n",
    "\n",
    "R = orthogonal_R(X_fit, Y_fit, center=True)\n",
    "\n",
    "# ---------- (d) Apply rotation to all image features ----------\n",
    "X_img_rot = X_img_norm @ R                      # still unit-norm? (R is orthogonal) -> preserve norms\n",
    "# Re-normalize to mitigate numeric drift:\n",
    "X_img_rot = X_img_rot / np.linalg.norm(X_img_rot, axis=1, keepdims=True)\n",
    "\n",
    "# ---------- Helpers: projection & plotting ----------\n",
    "def project_2d(X, seed=SEED):\n",
    "    if USE_UMAP and HAVE_UMAP:\n",
    "        reducer = umap.UMAP(n_neighbors=20, min_dist=0.1, metric=\"cosine\", random_state=seed)\n",
    "        return reducer.fit_transform(X)\n",
    "    Xp = PCA(n_components=min(50, X.shape[1]), random_state=seed).fit_transform(X)\n",
    "    tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=seed)\n",
    "    return tsne.fit_transform(Xp)\n",
    "\n",
    "def plot_overlay(Z_img, Z_txt_rep, labels, title, savepath=None):\n",
    "    plt.figure(figsize=(7.2, 6.4))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "    for c in range(10):\n",
    "        m = (labels == c)\n",
    "        plt.scatter(Z_img[m,0], Z_img[m,1], s=12, alpha=0.7, c=[cmap(c)], label=f\"{CLASSES[c]} (img)\")\n",
    "        plt.scatter(Z_txt_rep[m,0], Z_txt_rep[m,1], s=26, alpha=0.9, c=[cmap(c)], marker=\"x\", label=f\"{CLASSES[c]} (text)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(ncol=2, fontsize=8, frameon=False)\n",
    "    plt.tight_layout()\n",
    "    if savepath: plt.savefig(savepath, dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "# ---------- (e) Visualize: BEFORE vs AFTER alignment ----------\n",
    "Z_img_before = project_2d(X_img_norm)\n",
    "Z_img_after  = project_2d(X_img_rot)\n",
    "Z_txt_rep    = project_2d(X_txt_norm_rep)\n",
    "\n",
    "plot_overlay(Z_img_before, Z_txt_rep, y, \"Before Procrustes: image vs text (2D)\")\n",
    "plot_overlay(Z_img_after,  Z_txt_rep, y, \"After Procrustes: image vs text (2D)\")\n",
    "\n",
    "# ---------- (f) Accuracy: BEFORE vs AFTER ----------\n",
    "# Zero-shot: compare image features to PER-CLASS text prototypes\n",
    "def top1_accuracy(X_img_feats, class_text_norm, labels):\n",
    "    # cosine logits = dot because both are L2-normalized\n",
    "    logits = X_img_feats @ class_text_norm.T             # [N, C]\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# (1) Normalize class_text (already unit) and image feats (we ensured unit)\n",
    "acc_before = top1_accuracy(X_img_norm, class_text_norm, y)\n",
    "acc_after  = top1_accuracy(X_img_rot,  class_text_norm, y)\n",
    "\n",
    "# Some quick “gap” metrics\n",
    "def cosine(a, b):\n",
    "    a = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b = b / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return np.sum(a*b, axis=1)\n",
    "\n",
    "matched_text = class_text_norm[y]\n",
    "pos_cos_before = cosine(X_img_norm, matched_text)\n",
    "pos_cos_after  = cosine(X_img_rot,  matched_text)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "neg_labels = np.array([rng.choice([c for c in range(10) if c != yy]) for yy in y])\n",
    "neg_text = class_text_norm[neg_labels]\n",
    "neg_cos_before = cosine(X_img_norm, neg_text)\n",
    "neg_cos_after  = cosine(X_img_rot,  neg_text)\n",
    "\n",
    "print(\"\\n=== Procrustes Alignment: Summary ===\")\n",
    "print(f\"Top-1 accuracy (before): {acc_before*100:.2f}%\")\n",
    "print(f\"Top-1 accuracy (after) : {acc_after*100:.2f}%\")\n",
    "print(f\"Mean cos (matched) before: {pos_cos_before.mean():.3f} | after: {pos_cos_after.mean():.3f}\")\n",
    "print(f\"Mean cos (mismatched) before: {neg_cos_before.mean():.3f} | after: {neg_cos_after.mean():.3f}\")\n",
    "print(f\"Avg margin (matched - mismatched) before: {(pos_cos_before - neg_cos_before).mean():.3f}\")\n",
    "print(f\"Avg margin (matched - mismatched) after : {(pos_cos_after  - neg_cos_after ).mean():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atml_pa0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
