{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd1f4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from types import MethodType\n",
    "from torchvision.models.resnet import Bottleneck\n",
    "from sklearn.decomposition import PCA\n",
    "import os, time, copy, math, random\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdf294",
   "metadata": {},
   "source": [
    "### Baseline Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4193d",
   "metadata": {},
   "source": [
    "Loading Pretrained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df07c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet = torchvision.models.resnet152()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc44f7b",
   "metadata": {},
   "source": [
    "Downloading CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ced4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download CIFAR-10 training and test datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c025ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b99e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# CLASSES\n",
    "print(len(trainset.classes))\n",
    "print(trainset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700452ff",
   "metadata": {},
   "source": [
    "Modfying Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14676880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACING FINAL LAYER\n",
    "pretrained_resnet.fc = torch.nn.Linear(in_features=2048, out_features=10, bias=True) \n",
    "# FREEZING BACKBONE\n",
    "for param in pretrained_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in pretrained_resnet.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63768ce",
   "metadata": {},
   "source": [
    "Training New Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62f03cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device = \"cpu\"\n",
    "pretrained_resnet = pretrained_resnet.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_resnet.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    pretrained_resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = pretrained_resnet(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct / total\n",
    "    print(f\"Validation Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d74d9",
   "metadata": {},
   "source": [
    "### Residual Connections in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faae324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "             ReLU-91          [-1, 128, 28, 28]               0\n",
      "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-97          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-107          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
      "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
      "            ReLU-111          [-1, 128, 28, 28]               0\n",
      "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "            ReLU-114          [-1, 128, 28, 28]               0\n",
      "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-117          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
      "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
      "            ReLU-121          [-1, 256, 28, 28]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
      "            ReLU-316          [-1, 256, 14, 14]               0\n",
      "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-319         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
      "            ReLU-323          [-1, 256, 14, 14]               0\n",
      "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "            ReLU-326          [-1, 256, 14, 14]               0\n",
      "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-329         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
      "            ReLU-333          [-1, 256, 14, 14]               0\n",
      "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
      "            ReLU-336          [-1, 256, 14, 14]               0\n",
      "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-339         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
      "            ReLU-343          [-1, 256, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
      "            ReLU-346          [-1, 256, 14, 14]               0\n",
      "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-349         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
      "            ReLU-353          [-1, 256, 14, 14]               0\n",
      "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "            ReLU-356          [-1, 256, 14, 14]               0\n",
      "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-359         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
      "            ReLU-363          [-1, 256, 14, 14]               0\n",
      "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
      "            ReLU-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-369         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
      "            ReLU-373          [-1, 256, 14, 14]               0\n",
      "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-379         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "            ReLU-383          [-1, 256, 14, 14]               0\n",
      "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
      "            ReLU-386          [-1, 256, 14, 14]               0\n",
      "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-389         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "            ReLU-393          [-1, 256, 14, 14]               0\n",
      "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
      "            ReLU-396          [-1, 256, 14, 14]               0\n",
      "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-399         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
      "            ReLU-403          [-1, 256, 14, 14]               0\n",
      "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-409         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
      "            ReLU-413          [-1, 256, 14, 14]               0\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
      "            ReLU-416          [-1, 256, 14, 14]               0\n",
      "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-419         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
      "            ReLU-423          [-1, 256, 14, 14]               0\n",
      "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
      "            ReLU-426          [-1, 256, 14, 14]               0\n",
      "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-429         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
      "            ReLU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-439         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
      "            ReLU-446          [-1, 256, 14, 14]               0\n",
      "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-449         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
      "            ReLU-456          [-1, 256, 14, 14]               0\n",
      "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-459         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
      "            ReLU-463          [-1, 256, 14, 14]               0\n",
      "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
      "            ReLU-466          [-1, 256, 14, 14]               0\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-469         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
      "            ReLU-473          [-1, 256, 14, 14]               0\n",
      "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
      "            ReLU-476          [-1, 256, 14, 14]               0\n",
      "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-479         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-483          [-1, 512, 14, 14]               0\n",
      "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-486            [-1, 512, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-491           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-498            [-1, 512, 7, 7]               0\n",
      "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-501           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-505            [-1, 512, 7, 7]               0\n",
      "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-511           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 58,164,298\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 58,143,808\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 606.58\n",
      "Params size (MB): 221.88\n",
      "Estimated Total Size (MB): 829.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(pretrained_resnet,input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "389a562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forward():\n",
    "    def forward(self: Bottleneck, x):\n",
    "        out = self.conv1(x) \n",
    "        out = self.bn1(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = out\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    return forward\n",
    "\n",
    "targets = [\n",
    "    (\"layer2\", 3),   \n",
    "    (\"layer3\", 5),\n",
    "    (\"layer4\", 0),\n",
    "]\n",
    "\n",
    "for layer_name, idx in targets:\n",
    "    blk = getattr(pretrained_resnet, layer_name)[idx]\n",
    "    blk.forward = MethodType(make_forward(), blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    pretrained_resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = pretrained_resnet(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct / total\n",
    "    print(f\"Validation Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d2a7e",
   "metadata": {},
   "source": [
    "### Feature Hierarchies and Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b48ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet.eval().to(device)\n",
    "\n",
    "tap_points = {\n",
    "    \"early\":  pretrained_resnet.layer1[-1],\n",
    "    \"middle\": pretrained_resnet.layer3[-1],\n",
    "    \"late\":   pretrained_resnet.layer4[-1],  \n",
    "}\n",
    "\n",
    "buffers = {k: [] for k in tap_points}\n",
    "\n",
    "@torch.no_grad()\n",
    "def _make_hook(name):\n",
    "    def hook(_module, _inp, out):\n",
    "        if out.dim() == 4:\n",
    "            vec = out.reshape(out.size(0), -1)  # flatten [B,C,H,W] → [B, C*H*W]\n",
    "        else:\n",
    "            vec = out\n",
    "        buffers[name].append(vec.detach().cpu())\n",
    "    return hook\n",
    "\n",
    "handles = [tap_points[k].register_forward_hook(_make_hook(k)) for k in tap_points]\n",
    "\n",
    "labels_all = []\n",
    "with torch.no_grad():\n",
    "    for x, y in testloader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        _ = pretrained_resnet(x)  # triggers hooks\n",
    "        labels_all.append(y)\n",
    "labels_all = torch.cat(labels_all).numpy()\n",
    "\n",
    "# Concatenate buffers\n",
    "feats = {k: torch.cat(v).numpy() for k, v in buffers.items()}\n",
    "\n",
    "# Clean up hooks if you’re done collecting\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "def pca100(X):\n",
    "    return PCA(n_components=100, random_state=0).fit_transform(X) if X.shape[1] > 100 else X\n",
    "\n",
    "def umap_reduce(X):\n",
    "    Xr = pca100(X)\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"euclidean\", random_state=0)\n",
    "    return reducer.fit_transform(Xr)\n",
    "\n",
    "embeddings = {k: umap_reduce(v) for k, v in feats.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5046e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(Z, y, title):\n",
    "    plt.figure(figsize=(6.5, 5.5))\n",
    "    # Let matplotlib choose colors; 10 classes -> 10 colors\n",
    "    for c in np.unique(y):\n",
    "        idx = (y == c)\n",
    "        plt.scatter(Z[idx, 0], Z[idx, 1], s=6, alpha=0.7, label=trainset.classes[c])\n",
    "    plt.legend(markerscale=3, fontsize=8, loc='best', frameon=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for name, Z in embeddings.items():\n",
    "    plot_scatter(Z, labels_all, f\"{name.capitalize()} features UMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc4ad5",
   "metadata": {},
   "source": [
    "### Transfer Learning and Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a32f9",
   "metadata": {},
   "source": [
    "Fine-tune Classification Head on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc78d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pretrained_resnet = torchvision.models.resnet152(weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2)\n",
    "# REPLACING FINAL LAYER\n",
    "pretrained_resnet.fc = torch.nn.Linear(in_features=2048, out_features=10, bias=True) \n",
    "# FREEZING BACKBONE\n",
    "for param in pretrained_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in pretrained_resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    pretrained_resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = pretrained_resnet(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct / total\n",
    "    print(f\"Validation Acc: {val_acc:.2f}%\")\n",
    "# def create_resnet152(num_classes: int, pretrained: bool = True):\n",
    "#     # weights arg mirrors the tutorial’s pattern (ResNet18 there). We use ResNet152 here.\n",
    "#     weights = torchvision.models.ResNet152_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "#     model = torchvision.models.resnet152(weights=weights)\n",
    "#     in_feats = model.fc.in_features\n",
    "#     model.fc = nn.Linear(in_feats, num_classes)\n",
    "#     return model\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(model, loader, criterion):\n",
    "#     model.eval()\n",
    "#     running_loss, running_corrects, n = 0.0, 0, 0\n",
    "#     for inputs, labels in loader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         preds = outputs.argmax(dim=1)\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         running_corrects += (preds == labels).sum().item()\n",
    "#         n += inputs.size(0)\n",
    "#     return running_loss / n, running_corrects / n\n",
    "\n",
    "# def train(model, dataloaders, sizes, epochs=15, lr=0.001, step_size=7, gamma=0.1):\n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "#     scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "#     best_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f\"Epoch {epoch}/{epochs-1}\\n\" + \"-\"*20)\n",
    "\n",
    "#         # --- train ---\n",
    "#         model.train()\n",
    "#         run_loss, run_corrects, n = 0.0, 0, 0\n",
    "#         for inputs, labels in dataloaders[\"train\"]:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             preds = outputs.argmax(dim=1)\n",
    "#             run_loss += loss.item() * inputs.size(0)\n",
    "#             run_corrects += (preds == labels).sum().item()\n",
    "#             n += inputs.size(0)\n",
    "\n",
    "#         train_loss = run_loss / n\n",
    "#         train_acc  = run_corrects / n\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # --- val ---\n",
    "#         val_loss, val_acc = evaluate(model, dataloaders[\"val\"], criterion)\n",
    "\n",
    "#         print(f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "#               f\"val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "\n",
    "#         if val_acc > best_acc:\n",
    "#             best_acc = val_acc\n",
    "#             best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#     print(f\"Best val acc: {best_acc:.4f}\")\n",
    "#     model.load_state_dict(best_wts)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730851d5",
   "metadata": {},
   "source": [
    "Train from scratch on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ecc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_resnet152_transfer.py\n",
    "import os, copy, random, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Data\n",
    "# =========================\n",
    "def build_dataloaders(\n",
    "    data_dir: str,\n",
    "    img_size: int = 224,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 4,\n",
    ") -> Tuple[Dict, Dict, List[str], int]:\n",
    "    \"\"\"\n",
    "    Expected structure:\n",
    "      data_dir/\n",
    "        train/<class>/*.jpg\n",
    "        val/<class>/*.jpg\n",
    "    \"\"\"\n",
    "    tfm_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    tfm_val = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    image_datasets = {\n",
    "        \"train\": datasets.ImageFolder(os.path.join(data_dir, \"train\"), tfm_train),\n",
    "        \"val\":   datasets.ImageFolder(os.path.join(data_dir, \"val\"), tfm_val),\n",
    "    }\n",
    "    loaders = {\n",
    "        split: torch.utils.data.DataLoader(\n",
    "            image_datasets[split],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(split==\"train\"),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True)\n",
    "        for split in [\"train\",\"val\"]\n",
    "    }\n",
    "    sizes = {k: len(v) for k, v in image_datasets.items()}\n",
    "    classes = image_datasets[\"train\"].classes\n",
    "    num_classes = len(classes)\n",
    "    return loaders, sizes, classes, num_classes\n",
    "\n",
    "# =========================\n",
    "# Model factory\n",
    "# =========================\n",
    "def make_model(num_classes: int, init: str):\n",
    "    \"\"\"\n",
    "    init: 'pretrained' or 'scratch'\n",
    "    \"\"\"\n",
    "    assert init in {\"pretrained\",\"scratch\"}\n",
    "    weights = models.ResNet152_Weights.IMAGENET1K_V2 if init==\"pretrained\" else None\n",
    "    m = models.resnet152(weights=weights)\n",
    "    in_features = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "def set_trainable_strategy(model: nn.Module, strategy: str):\n",
    "    \"\"\"\n",
    "    strategy: 'full' or 'final_block'\n",
    "      - 'full': all layers trainable\n",
    "      - 'final_block': freeze everything except layer4 and fc\n",
    "    Returns a list of parameter groups for the optimizer.\n",
    "    \"\"\"\n",
    "    assert strategy in {\"full\",\"final_block\"}\n",
    "    # First, freeze everything\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if strategy == \"full\":\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        # single param group\n",
    "        params = [{\"params\": model.parameters()}]\n",
    "    else:\n",
    "        # Unfreeze only layer4 + fc\n",
    "        for p in model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.fc.parameters():\n",
    "            p.requires_grad = True\n",
    "        # You can give head a slightly higher LR if desired; keep same for fairness here\n",
    "        params = [\n",
    "            {\"params\": model.layer4.parameters()}, \n",
    "            {\"params\": model.fc.parameters()}\n",
    "        ]\n",
    "    return params\n",
    "\n",
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader, criterion):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct  += (pred == y).sum().item()\n",
    "        total    += x.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "def train_one(\n",
    "    model: nn.Module,\n",
    "    loaders: Dict,\n",
    "    max_epochs: int = 15,\n",
    "    base_lr: float = 1e-3,\n",
    "    step_size: int = 7,\n",
    "    gamma: float = 0.1,\n",
    "    params_override=None,\n",
    "):\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if params_override is None:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.SGD(params_override, lr=base_lr, momentum=0.9)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    best = {\"val_acc\": 0.0, \"epoch\": -1, \"state\": copy.deepcopy(model.state_dict())}\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        run_loss, run_corrects, n = 0.0, 0, 0\n",
    "\n",
    "        for x, y in loaders[\"train\"]:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = logits.argmax(dim=1)\n",
    "            run_loss     += loss.item() * x.size(0)\n",
    "            run_corrects += (pred == y).sum().item()\n",
    "            n            += x.size(0)\n",
    "\n",
    "        train_loss = run_loss / n\n",
    "        train_acc  = run_corrects / n\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, loaders[\"val\"], criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: \"\n",
    "              f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best[\"val_acc\"]:\n",
    "            best[\"val_acc\"] = val_acc\n",
    "            best[\"epoch\"]   = epoch\n",
    "            best[\"state\"]   = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # restore best\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# Experiment runner (b + c)\n",
    "# =========================\n",
    "def run_experiments(\n",
    "    data_dir: str,\n",
    "    img_size: int = 224,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 4,\n",
    "    epochs: int = 15,\n",
    "    base_lr: float = 1e-3,\n",
    "):\n",
    "    set_seed(42)\n",
    "    loaders, sizes, classes, num_classes = build_dataloaders(\n",
    "        data_dir, img_size, batch_size, num_workers\n",
    "    )\n",
    "    print(f\"Classes ({num_classes}): {classes}\")\n",
    "\n",
    "    configs = [\n",
    "        {\"init\":\"pretrained\", \"strategy\":\"full\"},\n",
    "        {\"init\":\"pretrained\", \"strategy\":\"final_block\"},\n",
    "        {\"init\":\"scratch\",    \"strategy\":\"full\"},\n",
    "        {\"init\":\"scratch\",    \"strategy\":\"final_block\"},\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for cfg in configs:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Config: init={cfg['init']} | strategy={cfg['strategy']}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        model = make_model(num_classes, init=cfg[\"init\"])\n",
    "        param_groups = set_trainable_strategy(model, strategy=cfg[\"strategy\"])\n",
    "        best = train_one(\n",
    "            model,\n",
    "            loaders,\n",
    "            max_epochs=epochs,\n",
    "            base_lr=base_lr,\n",
    "            params_override=param_groups\n",
    "        )\n",
    "        # Save checkpoint for each run\n",
    "        Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "        ckpt_name = f\"resnet152_{cfg['init']}_{cfg['strategy']}.pt\"\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{ckpt_name}\")\n",
    "        print(f\"Saved: checkpoints/{ckpt_name}\")\n",
    "\n",
    "        results.append({\n",
    "            \"init\": cfg[\"init\"],\n",
    "            \"strategy\": cfg[\"strategy\"],\n",
    "            \"best_val_acc\": best[\"val_acc\"],\n",
    "            \"best_epoch\": best[\"epoch\"],\n",
    "        })\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\n==== Summary (higher is better) ====\")\n",
    "    print(f\"{'init':<12} {'strategy':<12} {'best_acc':>9} {'epoch':>7}\")\n",
    "    for r in results:\n",
    "        print(f\"{r['init']:<12} {r['strategy']:<12} {r['best_val_acc']:>9.4f} {r['best_epoch']:>7d}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CHANGE THIS to your dataset root (not ImageNet). Example mirrors PyTorch tutorial layout.\n",
    "    DATA_DIR = \"data/hymenoptera_data\"   # e.g., ants & bees tutorial dataset\n",
    "    run_experiments(\n",
    "        data_dir=DATA_DIR,\n",
    "        img_size=224,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        epochs=15,\n",
    "        base_lr=1e-3,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
